{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Creating the Synthetic Dataset\n",
    "1.1. Define the Dataset Structure\n",
    "We'll create three main components:\n",
    "\n",
    "Customers:\n",
    "\n",
    "customer_id: Unique identifier.\n",
    "age: Age of the customer.\n",
    "gender: Gender (Male, Female, Other).\n",
    "skin_type: Skin type (Oily, Dry, Combination, Sensitive).\n",
    "skin_concerns: Primary skin concerns (e.g., Acne, Wrinkles).\n",
    "Products:\n",
    "\n",
    "product_id: Unique identifier.\n",
    "product_name: Name of the product.\n",
    "brand: Brand name.\n",
    "category: Product category (Cleanser, Moisturizer, Serum, etc.).\n",
    "ingredients: Key ingredients.\n",
    "benefits: Primary benefits.\n",
    "suitable_skin_types: Suitable skin types.\n",
    "Interactions:\n",
    "\n",
    "interaction_id: Unique identifier.\n",
    "customer_id: Reference to the customer.\n",
    "product_id: Reference to the product.\n",
    "rating: Rating given by the customer (1 to 5).\n",
    "review: Short review text.\n",
    "1.2. Generate Synthetic Data\n",
    "We'll use Python with the pandas, numpy, and faker libraries to generate this data. You can run the following script in a Jupyter Notebook or Google Colab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic dataset created and saved as CSV files.\n"
     ]
    }
   ],
   "source": [
    "#a. Install Necessary Libraries\n",
    "#If you're using Google Colab, most libraries are pre-installed. Otherwise, install them using pip:\n",
    "#!pip install pandas numpy faker\n",
    "\n",
    "#b. Python Script to Generate Synthetic Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Seed for reproducibility\n",
    "Faker.seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Define sample data\n",
    "genders = ['Male', 'Female', 'Other']\n",
    "skin_types = ['Oily', 'Dry', 'Combination', 'Sensitive']\n",
    "skin_concerns_list = ['Acne', 'Wrinkles', 'Dark Spots', 'Dryness', 'Redness', 'Uneven Texture']\n",
    "categories = ['Cleanser', 'Moisturizer', 'Serum', 'Sunscreen', 'Exfoliator', 'Mask']\n",
    "brands = ['BrandA', 'BrandB', 'BrandC', 'BrandD', 'BrandE']\n",
    "ingredients = ['Hyaluronic Acid', 'Salicylic Acid', 'Vitamin C', 'Retinol', 'Niacinamide', 'Glycolic Acid']\n",
    "benefits = ['Hydration', 'Oil Control', 'Brightening', 'Anti-Aging', 'Exfoliation', 'Soothing']\n",
    "\n",
    "# Generate Customers Data\n",
    "num_customers = 1000\n",
    "customers = []\n",
    "for i in range(1, num_customers + 1):\n",
    "    customer = {\n",
    "        'customer_id': i,\n",
    "        'age': np.random.randint(18, 65),\n",
    "        'gender': random.choice(genders),\n",
    "        'skin_type': random.choice(skin_types),\n",
    "        'skin_concerns': ', '.join(random.sample(skin_concerns_list, k=random.randint(1,3)))\n",
    "    }\n",
    "    customers.append(customer)\n",
    "\n",
    "customers_df = pd.DataFrame(customers)\n",
    "\n",
    "# Generate Products Data\n",
    "num_products = 200\n",
    "products = []\n",
    "for i in range(1, num_products + 1):\n",
    "    product = {\n",
    "        'product_id': i,\n",
    "        'product_name': f\"{random.choice(['Ultra', 'Hydra', 'Clear', 'Radiant', 'Pure'])} {random.choice(['Glow', 'Fresh', 'Smooth', 'Bright', 'Revive'])} {random.randint(100,999)}\",\n",
    "        'brand': random.choice(brands),\n",
    "        'category': random.choice(categories),\n",
    "        'ingredients': ', '.join(random.sample(ingredients, k=random.randint(2,4))),\n",
    "        'benefits': ', '.join(random.sample(benefits, k=random.randint(1,3))),\n",
    "        'suitable_skin_types': ', '.join(random.sample(skin_types, k=random.randint(1,4)))\n",
    "    }\n",
    "    products.append(product)\n",
    "\n",
    "products_df = pd.DataFrame(products)\n",
    "\n",
    "# Generate Interactions Data\n",
    "interactions = []\n",
    "interaction_id = 1\n",
    "for _ in range(5000):  # 5 interactions per customer on average\n",
    "    customer = random.choice(customers_df['customer_id'].tolist())\n",
    "    product = random.choice(products_df['product_id'].tolist())\n",
    "    rating = random.randint(1, 5)\n",
    "    review = fake.sentence(nb_words=10)\n",
    "    interaction = {\n",
    "        'interaction_id': interaction_id,\n",
    "        'customer_id': customer,\n",
    "        'product_id': product,\n",
    "        'rating': rating,\n",
    "        'review': review\n",
    "    }\n",
    "    interactions.append(interaction)\n",
    "    interaction_id += 1\n",
    "\n",
    "interactions_df = pd.DataFrame(interactions)\n",
    "\n",
    "# Save to CSV\n",
    "customers_df.to_csv('customers.csv', index=False)\n",
    "products_df.to_csv('products.csv', index=False)\n",
    "interactions_df.to_csv('interactions.csv', index=False)\n",
    "\n",
    "print(\"Synthetic dataset created and saved as CSV files.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Explanation of the Script\n",
    "Libraries:\n",
    "\n",
    "pandas: For data manipulation.\n",
    "numpy: For numerical operations.\n",
    "faker: To generate realistic fake data.\n",
    "random: For random selections.\n",
    "Data Definitions:\n",
    "\n",
    "Define lists of possible values for genders, skin types, skin concerns, product categories, brands, ingredients, and benefits.\n",
    "Customers Data:\n",
    "\n",
    "Generate 1,000 customers with random ages (18-65), genders, skin types, and 1-3 skin concerns.\n",
    "Products Data:\n",
    "\n",
    "Generate 200 skincare products with random names, brands, categories, 2-4 ingredients, 1-3 benefits, and suitable skin types.\n",
    "Interactions Data:\n",
    "\n",
    "Simulate 5,000 interactions where customers rate and review products. Each interaction links a customer to a product with a rating (1-5) and a short review.\n",
    "Saving Data:\n",
    "\n",
    "Save the generated data into CSV files: customers.csv, products.csv, and interactions.csv.\n",
    "d. Sample Data Preview\n",
    "Customers (customers.csv):\n",
    "\n",
    "customer_id\tage\tgender\tskin_type\tskin_concerns\n",
    "1\t25\tFemale\tCombination\tAcne, Wrinkles\n",
    "2\t34\tMale\tOily\tDark Spots\n",
    "...\t...\t...\t...\t...\n",
    "Products (products.csv):\n",
    "\n",
    "product_id\tproduct_name\tbrand\tcategory\tingredients\tbenefits\tsuitable_skin_types\n",
    "1\tUltra Glow 123\tBrandA\tCleanser\tHyaluronic Acid, Vitamin C\tHydration\tOily, Dry\n",
    "2\tHydra Fresh 456\tBrandB\tMoisturizer\tSalicylic Acid, Retinol, Niacinamide\tOil Control, Anti-Aging\tOily, Combination\n",
    "...\t...\t...\t...\t...\t...\t...\n",
    "Interactions (interactions.csv):\n",
    "\n",
    "interaction_id\tcustomer_id\tproduct_id\trating\treview\n",
    "1\t1\t5\t4\t\"This product works well.\"\n",
    "2\t2\t3\t5\t\"Loved the results after use.\"\n",
    "...\t...\t...\t...\t...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Building and Training the Neural Network Using PyTorch\n",
    "We'll build a neural network using PyTorch to predict whether a customer will like a product (liked) based on their profile and product attributes. We'll follow these steps:\n",
    "\n",
    "Data Preprocessing\n",
    "Dataset and DataLoader Creation\n",
    "Neural Network Definition\n",
    "Training the Model\n",
    "Evaluating the Model\n",
    "Saving and Uploading the Model to Hugging Face\n",
    "2.1. Setting Up the Environment\n",
    "Ensure you have the necessary libraries installed. If you're using Google Colab, most are pre-installed. Otherwise, install them using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas numpy scikit-learn torch torchvision transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 24), (1000, 24), (4000,), (1000,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.2. Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#2.3. Load and Preprocess the Data\n",
    "# Load datasets\n",
    "customers = pd.read_csv('customers.csv')\n",
    "products = pd.read_csv('products.csv')\n",
    "interactions = pd.read_csv('interactions.csv')\n",
    "\n",
    "# Merge interactions with customers and products\n",
    "data = interactions.merge(customers, on='customer_id').merge(products, on='product_id')\n",
    "\n",
    "# Create binary target variable\n",
    "data['liked'] = data['rating'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(['interaction_id', 'rating', 'review'], axis=1)\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "# Initialize LabelEncoders\n",
    "le_gender = LabelEncoder()\n",
    "le_skin_type = LabelEncoder()\n",
    "le_category = LabelEncoder()\n",
    "le_brand = LabelEncoder()\n",
    "\n",
    "# Encode categorical features\n",
    "data['gender_encoded'] = le_gender.fit_transform(data['gender'])\n",
    "data['skin_type_encoded'] = le_skin_type.fit_transform(data['skin_type'])\n",
    "data['category_encoded'] = le_category.fit_transform(data['category'])\n",
    "data['brand_encoded'] = le_brand.fit_transform(data['brand'])\n",
    "\n",
    "# Feature Counts for multi-valued fields\n",
    "data['skin_concerns_count'] = data['skin_concerns'].apply(lambda x: len(x.split(',')))\n",
    "data['suitable_skin_types_count'] = data['suitable_skin_types'].apply(lambda x: len(x.split(',')))\n",
    "\n",
    "# Text vectorization for ingredients and benefits\n",
    "cv_ingredients = CountVectorizer(max_features=100)\n",
    "cv_benefits = CountVectorizer(max_features=50)\n",
    "\n",
    "ingredients_matrix = cv_ingredients.fit_transform(data['ingredients'])\n",
    "benefits_matrix = cv_benefits.fit_transform(data['benefits'])\n",
    "\n",
    "# Convert to DataFrame\n",
    "ingredients_df = pd.DataFrame(ingredients_matrix.toarray(), columns=cv_ingredients.get_feature_names_out())\n",
    "benefits_df = pd.DataFrame(benefits_matrix.toarray(), columns=cv_benefits.get_feature_names_out())\n",
    "\n",
    "# Concatenate with original dataframe\n",
    "data = pd.concat([data, ingredients_df, benefits_df], axis=1)\n",
    "\n",
    "# Drop original text columns\n",
    "data = data.drop(['gender', 'skin_type', 'skin_concerns', 'category', 'brand', 'suitable_skin_types', 'product_name', 'ingredients', 'benefits'], axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['age', 'skin_concerns_count', 'suitable_skin_types_count'] + list(ingredients_df.columns) + list(benefits_df.columns)\n",
    "data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "# Define feature columns and target\n",
    "feature_columns = [col for col in data.columns if col != 'liked']\n",
    "\n",
    "X = data[feature_columns].values\n",
    "y = data['liked'].values\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of training data:\n",
      "[[ 2.88000000e+02  7.20000000e+01 -7.73528100e-01  1.00000000e+00\n",
      "   2.00000000e+00  1.21785578e+00  3.00000000e+00  1.00000000e+00\n",
      "  -1.35375500e+00]\n",
      " [ 8.34000000e+02  1.94000000e+02  9.87492047e-01  0.00000000e+00\n",
      "   0.00000000e+00  1.21785578e+00  5.00000000e+00  4.00000000e+00\n",
      "   5.29599200e-01]\n",
      " [ 7.40000000e+01  1.15000000e+02 -6.26776421e-01  1.00000000e+00\n",
      "   2.00000000e+00 -3.66456103e-03  0.00000000e+00  2.00000000e+00\n",
      "  -1.35375500e+00]\n",
      " [ 5.93000000e+02  5.00000000e+00 -1.50728649e+00  2.00000000e+00\n",
      "   0.00000000e+00 -1.22518490e+00  5.00000000e+00  3.00000000e+00\n",
      "  -4.12077898e-01]\n",
      " [ 2.27000000e+02  1.76000000e+02 -2.59897223e-01  1.00000000e+00\n",
      "   1.00000000e+00  1.21785578e+00  5.00000000e+00  3.00000000e+00\n",
      "  -4.12077898e-01]]\n",
      "\n",
      "Corresponding training labels:\n",
      "[0 0 1 1 0]\n",
      "\n",
      "Sample of test data:\n",
      "[[ 2.08000000e+02  1.35000000e+02  1.42774708e+00  0.00000000e+00\n",
      "   0.00000000e+00 -3.66456103e-03  0.00000000e+00  2.00000000e+00\n",
      "   1.47127630e+00]\n",
      " [ 4.20000000e+01  1.88000000e+02 -9.93655618e-01  0.00000000e+00\n",
      "   0.00000000e+00 -1.22518490e+00  4.00000000e+00  3.00000000e+00\n",
      "   5.29599200e-01]\n",
      " [ 2.02000000e+02  1.28000000e+02  1.57449876e+00  0.00000000e+00\n",
      "   3.00000000e+00 -1.22518490e+00  0.00000000e+00  0.00000000e+00\n",
      "  -1.35375500e+00]\n",
      " [ 8.31000000e+02  1.05000000e+02 -3.97697050e-02  2.00000000e+00\n",
      "   2.00000000e+00  1.21785578e+00  3.00000000e+00  0.00000000e+00\n",
      "  -1.35375500e+00]\n",
      " [ 9.51000000e+02  2.40000000e+01 -2.59897223e-01  0.00000000e+00\n",
      "   3.00000000e+00 -1.22518490e+00  5.00000000e+00  1.00000000e+00\n",
      "  -1.35375500e+00]]\n",
      "\n",
      "Corresponding test labels:\n",
      "[0 0 0 0 0]\n",
      "\n",
      "Feature names:\n",
      "['customer_id', 'product_id', 'age', 'gender_encoded', 'skin_type_encoded', 'skin_concerns_count', 'category_encoded', 'brand_encoded', 'suitable_skin_types_count']\n",
      "\n",
      "Shape of training data: (4000, 9)\n",
      "Shape of test data: (1000, 9)\n"
     ]
    }
   ],
   "source": [
    "# Display some train and test data\n",
    "print(\"Sample of training data:\")\n",
    "print(X_train[:5])\n",
    "print(\"\\nCorresponding training labels:\")\n",
    "print(y_train[:5])\n",
    "\n",
    "print(\"\\nSample of test data:\")\n",
    "print(X_test[:5])\n",
    "print(\"\\nCorresponding test labels:\")\n",
    "print(y_test[:5])\n",
    "\n",
    "print(\"\\nFeature names:\")\n",
    "print(feature_columns)\n",
    "\n",
    "print(\"\\nShape of training data:\", X_train.shape)\n",
    "print(\"Shape of test data:\", X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.4. Create a Custom Dataset Class\n",
    "class SkincareDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.X = torch.tensor(features, dtype=torch.float32)\n",
    "        self.y = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)  # For binary classification\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "#Create DataLoaders\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = SkincareDataset(X_train, y_train)\n",
    "test_dataset = SkincareDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from train_loader:\n",
      "Batch features shape: torch.Size([32, 24])\n",
      "Batch labels shape: torch.Size([32, 1])\n",
      "First few features in batch:\n",
      "tensor([[ 1.2700e+02,  3.6000e+01,  3.2711e-01,  1.0000e+00,  3.0000e+00,\n",
      "          3.0000e+00,  0.0000e+00, -3.6646e-03,  1.4713e+00, -8.1959e-01,\n",
      "         -1.0488e+00, -9.7745e-01,  1.0606e+00, -8.5201e-01,  9.0490e-01,\n",
      "          1.0276e+00,  1.2719e+00,  1.2719e+00, -7.0912e-01, -7.1359e-01,\n",
      "         -6.0816e-01, -6.2578e-01, -7.1359e-01, -7.0499e-01],\n",
      "        [ 1.5600e+02,  1.4400e+02, -1.0670e+00,  0.0000e+00,  3.0000e+00,\n",
      "          4.0000e+00,  2.0000e+00, -1.2252e+00, -1.3538e+00,  6.3824e-01,\n",
      "         -1.0488e+00,  1.0231e+00, -9.4283e-01, -8.5201e-01,  9.0490e-01,\n",
      "         -9.7316e-01,  1.2719e+00,  1.2719e+00, -7.0912e-01,  1.4014e+00,\n",
      "         -6.0816e-01, -6.2578e-01,  1.4014e+00,  1.4185e+00]])\n",
      "Corresponding labels:\n",
      "tensor([[0.],\n",
      "        [0.]])\n",
      "\n",
      "Sample from test_loader:\n",
      "Batch features shape: torch.Size([32, 24])\n",
      "Batch labels shape: torch.Size([32, 1])\n",
      "First few features in batch:\n",
      "tensor([[ 2.7300e+02,  2.0000e+01,  9.8749e-01,  0.0000e+00,  3.0000e+00,\n",
      "          1.0000e+00,  2.0000e+00, -1.2252e+00,  1.4713e+00,  2.0961e+00,\n",
      "          9.5348e-01,  1.0231e+00, -9.4283e-01, -8.5201e-01,  9.0490e-01,\n",
      "         -9.7316e-01, -7.8621e-01, -7.8621e-01, -7.0912e-01,  1.4014e+00,\n",
      "         -6.0816e-01, -6.2578e-01,  1.4014e+00, -7.0499e-01],\n",
      "        [ 2.7100e+02,  1.8400e+02,  3.2711e-01,  0.0000e+00,  0.0000e+00,\n",
      "          2.0000e+00,  0.0000e+00, -3.6646e-03, -4.1208e-01, -8.1959e-01,\n",
      "          9.5348e-01, -9.7745e-01, -9.4283e-01, -8.5201e-01, -1.1051e+00,\n",
      "          1.0276e+00, -7.8621e-01, -7.8621e-01, -7.0912e-01,  1.4014e+00,\n",
      "          1.6443e+00, -6.2578e-01,  1.4014e+00, -7.0499e-01]])\n",
      "Corresponding labels:\n",
      "tensor([[1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "# 2.5. Inspect DataLoader contents\n",
    "\n",
    "print(\"Sample from train_loader:\")\n",
    "for batch_features, batch_labels in train_loader:\n",
    "    print(\"Batch features shape:\", batch_features.shape)\n",
    "    print(\"Batch labels shape:\", batch_labels.shape)\n",
    "    print(\"First few features in batch:\")\n",
    "    print(batch_features[:2])\n",
    "    print(\"Corresponding labels:\")\n",
    "    print(batch_labels[:2])\n",
    "    break  # We only need to see one batch\n",
    "\n",
    "print(\"\\nSample from test_loader:\")\n",
    "for batch_features, batch_labels in test_loader:\n",
    "    print(\"Batch features shape:\", batch_features.shape)\n",
    "    print(\"Batch labels shape:\", batch_labels.shape)\n",
    "    print(\"First few features in batch:\")\n",
    "    print(batch_features[:2])\n",
    "    print(\"Corresponding labels:\")\n",
    "    print(batch_labels[:2])\n",
    "    break  # We only need to see one batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#2.6. Define the Neural Network\n",
    "class SkincareNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SkincareNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(self.fc5(x))\n",
    "        return x\n",
    "\n",
    "#2.7. Initialize the Model, Loss Function, and Optimizer\n",
    "input_size = X_train.shape[1]\n",
    "model = SkincareNN(input_size)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.7462, Test Accuracy: 60.20%\n",
      "Epoch [2/20], Loss: 0.7168, Test Accuracy: 60.20%\n",
      "Epoch [3/20], Loss: 0.7050, Test Accuracy: 60.20%\n",
      "Epoch [4/20], Loss: 0.7010, Test Accuracy: 60.20%\n",
      "Epoch [5/20], Loss: 0.6985, Test Accuracy: 60.20%\n",
      "Epoch [6/20], Loss: 0.6969, Test Accuracy: 60.20%\n",
      "Epoch [7/20], Loss: 0.6959, Test Accuracy: 60.20%\n",
      "Epoch [8/20], Loss: 0.6953, Test Accuracy: 60.20%\n",
      "Epoch [9/20], Loss: 0.6948, Test Accuracy: 60.20%\n",
      "Epoch [10/20], Loss: 0.6946, Test Accuracy: 60.20%\n",
      "Epoch [11/20], Loss: 0.6940, Test Accuracy: 60.20%\n",
      "Epoch [12/20], Loss: 0.6939, Test Accuracy: 60.20%\n",
      "Epoch [13/20], Loss: 0.6941, Test Accuracy: 60.20%\n",
      "Epoch [14/20], Loss: 0.6937, Test Accuracy: 60.20%\n",
      "Epoch [15/20], Loss: 0.6938, Test Accuracy: 60.20%\n",
      "Epoch [16/20], Loss: 0.6936, Test Accuracy: 60.20%\n",
      "Epoch [17/20], Loss: 0.6937, Test Accuracy: 60.20%\n",
      "Epoch [18/20], Loss: 0.6936, Test Accuracy: 60.20%\n",
      "Epoch [19/20], Loss: 0.6935, Test Accuracy: 60.20%\n",
      "Epoch [20/20], Loss: 0.6935, Test Accuracy: 60.20%\n"
     ]
    }
   ],
   "source": [
    "#2.8. Training the Model\n",
    "# Training Loop\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Evaluation on Test Set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs >= 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total * 100\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.9. Evaluate the Model\n",
    "# Final Evaluation\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs >= 0.5).float()\n",
    "        y_pred.extend(predicted.squeeze().tolist())\n",
    "        y_true.extend(labels.squeeze().tolist())\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.10. Save and Upload the Model to Hugging Face\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'skincare_nn.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
